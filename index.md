This page contains audio samples generated by our voice style transfer framework, created for our final project for the Audio Processing and Indexing 2021 course at leiden university. For this project we investigate voice style transfer using the [AutoVC](https://github.com/auspicious3000/autovc) framework.

The table below contains conversion samples between speakers from the VCTK dataset. We define the following elements:

## Vocoders:
* **WaveNet:** A [WaveNet](https://github.com/r9y9/wavenet_vocoder) vocoder pretrained on VCTK by the AutoVC authors.
* **MelGAN:** A Multi-band [MelGAN](https://github.com/kan-bayashi/ParallelWaveGAN) vocoder pretrained on VCTK
* **Griffin:** A [Griffin-Lim based](https://librosa.org/doc/main/generated/librosa.griffinlim.html) vocoder method.

## AutoVC Models:
* **AutoVC:** The pretrained AutoVC network uploaded by the authors.
* **New AutoVC:** Retrained AutoVC network with MelGAN spectograms.

The baseline model, provided by the authors, is **AutoVC+WaveNet**. 

<style>
	.alignright {
		text-align: right;
		float:right;
	}
</style>

We notice that **AutoVC + WaveNet** performs well on short samples with seen speakers. However, its conversion times are very long and its performance is very bad when the target speaker is unseen (sample p225 &rarr;Wouter AutoVC+WaveNet). Furthermore, the performance on longer audio-samples is quite bad (sample p226 &rarr; AutoVC+WaveNet) due to it being trained on samples of circa 2 seconds.

A solution to the 2-second limitation is the introduction of chunking, which divides the audio sample into chunks of ca. 2 seconds (see p226 &rarr; Wouter | AutoVC + WaveNet (chunking)), although this might introduce some 'choppyness' when cutting is done in the middle of sentences due to the context loss.

A first solution to the slow conversion time was to replace the WaveNet vocoder by a Griffin-Lim algorithm. This significantly improved conversion speed, at the cost of audio quality. We aimed to improve this by introducing the Multiband MelGAN model. 

The **New AutoVC + MelGAN** model was retrained on the full VCTK dataset, using the MelGAN-spectrogram format and longer-sized audio samples. This improves audio quality significantly compared to the Griffin-Lim algorithm, while reducing the Vocoder processing time by 99,96% compared to **(AutoVC +) WaveNet**


## Short Samples
| Source Speaker | Target Speaker | Results |
|----|----|----|
| p225 (Female) <br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001.wav'></audio> | p225 (Female) <br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001.wav'></audio> | AutoVC + WaveNet (Baseline)<span class='alignright'>(320.76s)</span> <br>  <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001xp225_old_wavenet.wav'></audio> AutoVC + Griffin <span class='alignright'>(1.19s)</span><br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001xp225_old_griffin.wav'></audio> <br> AutoVC + MelGAN <span class='alignright'>(1.07s)</span><br>  <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001xp225_old_melgan.wav'></audio> <br> New AutoVC + MelGAN <span class='alignright'>(0.80s)</span><br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001xp225_new_melgan.wav'></audio> |
| | p226 (Male) <br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p226_003.wav'></audio> | AutoVC + WaveNet <span class='alignright'>(306.57s)</span> <br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001xp226_old_wavenet.wav'></audio> <br> New AutoVC + MelGAN <span class='alignright'>(1.08s)</span> <br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001xp226_new_melgan.wav'></audio> |
| | Wouter (Male) - Unseen<br>  <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/Wouter_this_is_a_testsentence.wav'></audio> |AutoVC + WaveNet  <span class='alignright'>(313.75s)</span> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001xWouter_wavenet.wav'></audio> AutoVC + MelGAN  <span class='alignright'>(1.31s)</span> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001xWouter_new_melgan.wav'></audio>|

## Longer Samples
With longer input samples, the vanilla AutoVC model's output was scrambled after a few seconds. This issue was solved by dividing the input audio into chunks and processing those sequentially. 

The vanilla **AutoVC + WaveNet** model also performed very bad in zero-shot scenarios with unseen targets (see p226 &rarr; Wouter) due to the model being trained on relatively short audio samples of circa 2 seconds.


The **New AutoVC + MelGAN** model was trained on the full VCTK dataset, using longer audio-samples, which eliminated the need for chunking and enabled the model to function much better for unseen target speakers. 

| Source Speaker | Target Speaker | Results |
|----|----|----|
|p226 (Male)<br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p226_003.wav'></audio>| p225 (Female) <br>  <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001.wav'></audio>|AutoVC + WaveNet (no chunking) <span class='alignright'>(1039.64s)</span> <br> <audio controls><source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p226_003xp225_wavenet_no_chunking.wav'></audio> AutoVC + WaveNet (chunking) <span class='alignright'>(1040.37s)</span> <br> <audio controls><source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p226_003xp225_wavenet_chunked.wav'></audio> New AutoVC + MelGAN <span class='alignright'>(2.10s)</span><br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p226_003xp225_new_melgan.wav'>|
||Wouter (Male) - Unseen<br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/Wouter_this_is_a_testsentence.wav'></audio>| AutoVC + WaveNet <span class='alignright'>(905.23s)</span> <br> <audio controls><source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_003xWouter_wavenet.wav'></audio> New AutoVC + MelGAN <span class='alignright'>(1.92s)</span><br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p226_003xWouter_new_melgan.wav'></audio>|

## Unseen Source
	
Even for unseen source speakers, the **AutoVC + MelGAN** combination manages to generate decent samples. However, the voice style differences issue we described above remains.
	
| Source Speaker | Target Speaker | Results |
|----|----|----|
|Wouter (Male)<br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/Wouter_this_is_a_testsentence.wav'></audio>| p225 (Female) <br>  <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p225_001.wav'></audio>|New AutoVC + MelGAN <span class='alignright'>(1.390s)</span> <br> <audio controls><source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/6xp225_new_melgan.wav'></audio>|
||p226 (Male) <br> <audio controls> <source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/p226_003.wav'></audio>| New AutoVC + MelGAN <span class='alignright'>(1.152s)</span> <br> <audio controls><source src='https://raw.githubusercontent.com/Woutah/API/gh-pages/samples/6xp226_new_melgan.wav'></audio>|

